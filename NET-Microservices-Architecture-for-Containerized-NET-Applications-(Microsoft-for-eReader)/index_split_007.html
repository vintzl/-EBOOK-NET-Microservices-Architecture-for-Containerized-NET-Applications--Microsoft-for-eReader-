<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
  <head>
    <title>Unknown</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="id_ch6" class="block_21"> </p>
	<p id="id_Toc534713639" class="block_22">Designing and Developing Multi-Container and Microservice-Based .NET Applications</p>
	<p class="block_26">Developing containerized microservice applications means you are building multi-container applications. However, a multi-container application could also be simpler—for example, a three-tier application—and might not be built using a microservice architecture.</p>
	<p class="block_14">Earlier we raised the question “Is Docker necessary when building a microservice architecture?” The answer is a clear no. Docker is an enabler and can provide significant benefits, but containers and Docker are not a hard requirement for microservices. As an example, you could create a microservices-based application with or without Docker when using Azure Service Fabric, which supports microservices running as simple processes or as Docker containers.</p>
	<p class="block_14">However, if you know how to design and develop a microservices-based application that is also based on Docker containers, you will be able to design and develop any other, simpler application model. For example, you might design a three-tier application that also requires a multi-container approach. Because of that, and because microservice architectures are an important trend within the container world, this section focuses on a microservice architecture implementation using Docker containers.</p>
	<h1 id="id_Toc534713640" class="block_24">Designing a microservice-oriented application</h1>
	<p class="block_14">This section focuses on developing a hypothetical server-side enterprise application.</p>
	<h2 id="id_Toc534713641" class="block_18">Application specifications</h2>
	<p class="block_14">The hypothetical application handles requests by executing business logic, accessing databases, and then returning HTML, JSON, or XML responses. We will say that the application must support a variety of clients, including desktop browsers running Single Page Applications (SPAs), traditional web apps, mobile web apps, and native mobile apps. The application might also expose an API for third parties to consume. It should also be able to integrate its microservices or external applications asynchronously, so that approach will help resiliency of the microservices in the case of partial failures.</p>
	<p class="block_14">The application will consist of these types of components:</p>
	<ul class="list_">
	<li class="block_25">Presentation components. These are responsible for handling the UI and consuming remote services.</li>
	<li class="block_25">Domain or business logic. This is the application’s domain logic.</li>
	<li class="block_25">Database access logic. This consists of data access components responsible for accessing databases (SQL or NoSQL).</li>
	<li class="block_25">Application integration logic. This includes a messaging channel, mainly based on message brokers.</li>
</ul>
	<p class="block_14">The application will require high scalability, while allowing its vertical subsystems to scale out autonomously, because certain subsystems will require more scalability than others.</p>
	<p class="block_14">The application must be able to be deployed in multiple infrastructure environments (multiple public clouds and on-premises) and ideally should be cross-platform, able to move from Linux to Windows (or vice versa) easily.</p>
	<h2 id="id_Toc534713642" class="block_18">Development team context</h2>
	<p class="block_14">We also assume the following about the development process for the application:</p>
	<ul class="list_">
	<li class="block_25">You have multiple dev teams focusing on different business areas of the application.</li>
	<li class="block_25">New team members must become productive quickly, and the application must be easy to understand and modify.</li>
	<li class="block_25">The application will have a long-term evolution and ever-changing business rules.</li>
	<li class="block_25">You need good long-term maintainability, which means having agility when implementing new changes in the future while being able to update multiple subsystems with minimum impact on the other subsystems.</li>
	<li class="block_25">You want to practice continuous integration and continuous deployment of the application.</li>
	<li class="block_25">You want to take advantage of emerging technologies (frameworks, programming languages, etc.) while evolving the application. You do not want to make full migrations of the application when moving to new technologies, because that would result in high costs and impact the predictability and stability of the application.</li>
</ul>
	<h2 id="id_Toc534713643" class="block_18">Choosing an architecture</h2>
	<p class="block_14">What should the application deployment architecture be? The specifications for the application, along with the development context, strongly suggest that you should architect the application by decomposing it into autonomous subsystems in the form of collaborating microservices and containers, where a microservice is a container.</p>
	<p class="block_14">In this approach, each service (container) implements a set of cohesive and narrowly related functions. For example, an application might consist of services such as the catalog service, ordering service, basket service, user profile service, etc.</p>
	<p class="block_14">Microservices communicate using protocols such as HTTP (REST), but also asynchronously (for example, using AMQP) whenever possible, especially when propagating updates with integration events.</p>
	<p class="block_14">Microservices are developed and deployed as containers independently of one another. This means that a development team can be developing and deploying a certain microservice without impacting other subsystems.</p>
	<p class="block_14">Each microservice has its own database, allowing it to be fully decoupled from other microservices. When necessary, consistency between databases from different microservices is achieved using application-level integration events (through a logical event bus), as handled in Command and Query Responsibility Segregation (CQRS). Because of that, the business constraints must embrace eventual consistency between the multiple microservices and related databases.</p>
	<h3 id="id_Toc534713644" class="block_19">eShopOnContainers: A reference application for .NET Core and microservices deployed using containers</h3>
	<p class="block_17"><span class="text_5">So that you can focus on the architecture and technologies instead of thinking about a hypothetic business domain that you might not know, we have selected a well-known business domain—namely, a simplified e-commerce (e-shop) application that presents a catalog of products, takes orders from customers, verifies inventory, and performs other business functions. This container-based application source code is available in the </span><a href="https://aka.ms/MicroservicesArchitecture" class="text_4">eShopOnContainers</a><span class="text_5"> GitHub repo.</span></p>
	<p class="block_14">The application consists of multiple subsystems, including several store UI front ends (a Web application and a native mobile app), along with the back-end microservices and containers for all the required server-side operations with several API Gateways as consolidated entry points to the internal microservices. Figure 6-1 shows the architecture of the reference application.</p>
	<p class="block_14"><img src="images/image-54.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image1.png" class="calibre80"/></p>
	<p class="block_23"><span class="text_6">Figure 6-1</span><i class="calibre8">. The eShopOnContainers reference application architecture for development environment</i></p>
	<p class="block_14"><b class="calibre5">Hosting environment</b>. In Figure 6-1, you see several containers deployed within a single Docker host. That would be the case when deploying to a single Docker host with the docker-compose up command. However, if you are using an orchestrator or container cluster, each container could be running in a different host (node), and any node could be running any number of containers, as we explained earlier in the architecture section.</p>
	<p class="block_14"><b class="calibre5">Communication architecture</b>. The eShopOnContainers application uses two communication types, depending on the kind of the functional action (queries versus updates and transactions):</p>
	<ul class="list_">
	<li class="block_25">Http client-to-microservice communication through API Gateways. This is used for queries and when accepting update or transactional commands from the client apps. The approach using API Gateways is explained in detail in later sections.</li>
	<li class="block_25">Asynchronous event-based communication. This occurs through an event bus to propagate updates across microservices or to integrate with external applications. The event bus can be implemented with any messaging-broker infrastructure technology like RabbitMQ, or using higher-level (abstraction-level) service buses like Azure Service Bus, NServiceBus, MassTransit, or Brighter.</li>
</ul>
	<p class="block_14">The application is deployed as a set of microservices in the form of containers. Client apps can communicate with those microservices running as containers through the public URLs published by the API Gateways.</p>
	<h3 id="id_Toc534713645" class="block_19">Data sovereignty per microservice</h3>
	<p class="block_14">In the sample application, each microservice owns its own database or data source, although all SQL Server databases are deployed as a single container. This design decision was made only to make it easy for a developer to get the code from GitHub, clone it, and open it in Visual Studio or Visual Studio Code. Or alternatively, it makes it easy to compile the custom Docker images using .NET Core CLI and the Docker CLI, and then deploy and run them in a Docker development environment. Either way, using containers for data sources lets developers build and deploy in a matter of minutes without having to provision an external database or any other data source with hard dependencies on infrastructure (cloud or on-premises).</p>
	<p class="block_14">In a real production environment, for high availability and for scalability, the databases should be based on database servers in the cloud or on-premises, but not in containers.</p>
	<p class="block_14">Therefore, the units of deployment for microservices (and even for databases in this application) are Docker containers, and the reference application is a multi-container application that embraces microservices principles.</p>
	<h3 id="id_Toc534713646" class="block_19">Additional resources</h3>
	<ul class="list_">
	<li class="block_20"><span class="text_2">eShopOnContainers GitHub repo. Source code for the reference application</span><span class="text_"><br class="calibre6"/></span><a href="https://aka.ms/eShopOnContainers/" class="text_1">https://aka.ms/eShopOnContainers/</a></li>
</ul>
	<h2 id="id_Toc534713647" class="block_18">Benefits of a microservice-based solution</h2>
	<p class="block_14">A microservice based solution like this has many benefits:</p>
	<p class="block_14"><b class="calibre5">Each microservice is relatively small—easy to manage and evolve</b>. Specifically:</p>
	<ul class="list_">
	<li class="block_25">It is easy for a developer to understand and get started quickly with good productivity.</li>
	<li class="block_25">Containers start fast, which makes developers more productive.</li>
	<li class="block_25">An IDE like Visual Studio can load smaller projects fast, making developers productive.</li>
	<li class="block_25">Each microservice can be designed, developed, and deployed independently of other microservices, which provides agility because it is easier to deploy new versions of microservices frequently.</li>
</ul>
	<p class="block_14"><b class="calibre5">It is possible to scale out individual areas of the application</b>. For instance, the catalog service or the basket service might need to be scaled out, but not the ordering process. A microservices infrastructure will be much more efficient with regard to the resources used when scaling out than a monolithic architecture would be.</p>
	<p class="block_14"><b class="calibre5">You can divide the development work between multiple teams</b>. Each service can be owned by a single development team. Each team can manage, develop, deploy, and scale their service independently of the rest of the teams.</p>
	<p class="block_14"><b class="calibre5">Issues are more isolated</b>. If there is an issue in one service, only that service is initially impacted (except when the wrong design is used, with direct dependencies between microservices), and other services can continue to handle requests. In contrast, one malfunctioning component in a monolithic deployment architecture can bring down the entire system, especially when it involves resources, such as a memory leak. Additionally, when an issue in a microservice is resolved, you can deploy just the affected microservice without impacting the rest of the application.</p>
	<p class="block_14"><b class="calibre5">You can use the latest technologies</b>. Because you can start developing services independently and run them side by side (thanks to containers and .NET Core), you can start using the latest technologies and frameworks expediently instead of being stuck on an older stack or framework for the whole application.</p>
	<h2 id="id_Toc534713648" class="block_18">Downsides of a microservice-based solution</h2>
	<p class="block_14">A microservice based solution like this also has some drawbacks:</p>
	<p class="block_14"><b class="calibre5">Distributed application</b>. Distributing the application adds complexity for developers when they are designing and building the services. For example, developers must implement interservice communication using protocols like HTTP or AMPQ, which adds complexity for testing and exception handling. It also adds latency to the system.</p>
	<p class="block_14"><b class="calibre5">Deployment complexity</b>. An application that has dozens of microservices types and needs high scalability (it needs to be able to create many instances per service and balance those services across many hosts) means a high degree of deployment complexity for IT operations and management. If you are not using a microservice-oriented infrastructure (like an orchestrator and scheduler), that additional complexity can require far more development efforts than the business application itself.</p>
	<p class="block_14"><b class="calibre5">Atomic transactions</b>. Atomic transactions between multiple microservices usually are not possible. The business requirements have to embrace eventual consistency between multiple microservices.</p>
	<p class="block_14"><b class="calibre5">Increased global resource needs</b> (total memory, drives, and network resources for all the servers or hosts). In many cases, when you replace a monolithic application with a microservices approach, the amount of initial global resources needed by the new microservice-based application will be larger than the infrastructure needs of the original monolithic application. This is because the higher degree of granularity and distributed services requires more global resources. However, given the low cost of resources in general and the benefit of being able to scale out just certain areas of the application compared to long-term costs when evolving monolithic applications, the increased use of resources is usually a good tradeoff for large, long-term applications.</p>
	<p class="block_14"><b class="calibre5">Issues with direct client-to-microservice communication</b>. When the application is large, with dozens of microservices, there are challenges and limitations if the application requires direct client-to-microservice communications. One problem is a potential mismatch between the needs of the client and the APIs exposed by each of the microservices. In certain cases, the client application might need to make many separate requests to compose the UI, which can be inefficient over the Internet and would be impractical over a mobile network. Therefore, requests from the client application to the back-end system should be minimized.</p>
	<p class="block_14">Another problem with direct client-to-microservice communications is that some microservices might be using protocols that are not Web-friendly. One service might use a binary protocol, while another service might use AMQP messaging. Those protocols are not firewall-friendly and are best used internally. Usually, an application should use protocols such as HTTP and WebSockets for communication outside of the firewall.</p>
	<p class="block_14">Yet another drawback with this direct client-to-service approach is that it makes it difficult to refactor the contracts for those microservices. Over time developers might want to change how the system is partitioned into services. For example, they might merge two services or split a service into two or more services. However, if clients communicate directly with the services, performing this kind of refactoring can break compatibility with client apps.</p>
	<p class="block_14">As mentioned in the architecture section, when designing and building a complex application based on microservices, you might consider the use of multiple fine-grained API Gateways instead of the simpler direct client-to-microservice communication approach.</p>
	<p class="block_14"><b class="calibre5">Partitioning the microservices</b>. Finally, no matter which approach you take for your microservice architecture, another challenge is deciding how to partition an end-to-end application into multiple microservices. As noted in the architecture section of the guide, there are several techniques and approaches you can take. Basically, you need to identify areas of the application that are decoupled from the other areas and that have a low number of hard dependencies. In many cases, this is aligned to partitioning services by use case. For example, in our e-shop application, we have an ordering service that is responsible for all the business logic related to the order process. We also have the catalog service and the basket service that implement other capabilities. Ideally, each service should have only a small set of responsibilities. This is similar to the single responsibility principle (SRP) applied to classes, which states that a class should only have one reason to change. But in this case, it is about microservices, so the scope will be larger than a single class. Most of all, a microservice has to be completely autonomous, end to end, including responsibility for its own data sources.</p>
	<h2 id="id_Toc534713649" class="block_18">External versus internal architecture and design patterns</h2>
	<p class="block_14">The external architecture is the microservice architecture composed by multiple services, following the principles described in the architecture section of this guide. However, depending on the nature of each microservice, and independently of high-level microservice architecture you choose, it is common and sometimes advisable to have different internal architectures, each based on different patterns, for different microservices. The microservices can even use different technologies and programming languages. Figure 6-2 illustrates this diversity.</p>
	<p class="block_14"><img src="images/image-55.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image2.png" class="calibre27"/></p>
	<p class="block_23"><span class="text_6">Figure 6-2</span><i class="calibre8">. External versus internal architecture and design</i></p>
	<p class="block_14">For instance, in our <i class="calibre15">eShopOnContainers</i> sample, the catalog, basket, and user profile microservices are simple (basically, CRUD subsystems). Therefore, their internal architecture and design is straightforward. However, you might have other microservices, such as the ordering microservice, which is more complex and represents ever-changing business rules with a high degree of domain complexity. In cases like these, you might want to implement more advanced patterns within a particular microservice, like the ones defined with domain-driven design (DDD) approaches, as we are doing in the <i class="calibre15">eShopOnContainers</i> ordering microservice. (We will review these DDD patterns in the section later that explains the implementation of the <i class="calibre15">eShopOnContainers</i> ordering microservice.)</p>
	<p class="block_14">Another reason for a different technology per microservice might be the nature of each microservice. For example, it might be better to use a functional programming language like F#, or even a language like R if you are targeting AI and machine learning domains, instead of a more object-oriented programming language like C#.</p>
	<p class="block_14">The bottom line is that each microservice can have a different internal architecture based on different design patterns. Not all microservices should be implemented using advanced DDD patterns, because that would be over-engineering them. Similarly, complex microservices with ever-changing business logic should not be implemented as CRUD components, or you can end up with low-quality code.</p>
	<h2 id="id_Toc534713650" class="block_18">The new world: multiple architectural patterns and polyglot microservices</h2>
	<p class="block_14">There are many architectural patterns used by software architects and developers. The following are a few (mixing architecture styles and architecture patterns):</p>
	<ul class="list_">
	<li class="block_25">Simple CRUD, single-tier, single-layer.</li>
	<li class="block_20"><a href="https://msdn.microsoft.com/library/ee658109.aspx" class="text_1">Traditional N-Layered</a><span class="text_">.</span></li>
	<li class="block_20"><a href="https://blogs.msdn.microsoft.com/cesardelatorre/2011/07/03/published-first-alpha-version-of-domain-oriented-n-layered-architecture-v2-0/" class="text_1">Domain-Driven Design N-layered</a><span class="text_">.</span></li>
	<li class="block_20"><a href="https://8thlight.com/blog/uncle-bob/2012/08/13/the-clean-architecture.html" class="text_1">Clean Architecture</a><span class="text_"> (as used with </span><a href="https://aka.ms/WebAppArchitecture" class="text_1">eShopOnWeb</a><span class="text_">)</span></li>
	<li class="block_20"><a href="https://martinfowler.com/bliki/CQRS.html" class="text_1">Command and Query Responsibility Segregation</a><span class="text_"> (CQRS).</span></li>
	<li class="block_20"><a href="https://en.wikipedia.org/wiki/Event-driven_architecture" class="text_1">Event-Driven Architecture</a><span class="text_"> (EDA).</span></li>
</ul>
	<p class="block_14">You can also build microservices with many technologies and languages, such as ASP.NET Core Web APIs, NancyFx, ASP.NET Core SignalR (available with .NET Core 2), F#, Node.js, Python, Java, C++, GoLang, and more.</p>
	<p class="block_14">The important point is that no particular architecture pattern or style, nor any particular technology, is right for all situations. Figure 6-3 shows some approaches and technologies (although not in any particular order) that could be used in different microservices.</p>
	<p class="block_14"><img src="images/image-56.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image3.png" class="calibre81"/></p>
	<p class="block_23"><span class="text_6">Figure 6-3</span><i class="calibre8">. Multi-architectural patterns and the polyglot microservices world</i></p>
	<p class="block_14">As shown in Figure 6-3, in applications composed of many microservices (Bounded Contexts in domain-driven design terminology, or simply “subsystems” as autonomous microservices), you might implement each microservice in a different way. Each might have a different architecture pattern and use different languages and databases depending on the application’s nature, business requirements, and priorities. In some cases, the microservices might be similar. But that is not usually the case, because each subsystem’s context boundary and requirements are usually different.</p>
	<p class="block_14">For instance, for a simple CRUD maintenance application, it might not make sense to design and implement DDD patterns. But for your core domain or core business, you might need to apply more advanced patterns to tackle business complexity with ever-changing business rules.</p>
	<p class="block_14">Especially when you deal with large applications composed by multiple sub-systems, you should not apply a single top-level architecture based on a single architecture pattern. For instance, CQRS should not be applied as a top-level architecture for a whole application, but might be useful for a specific set of services.</p>
	<p class="block_14">There is no silver bullet or a right architecture pattern for every given case. You cannot have “one architecture pattern to rule them all.” Depending on the priorities of each microservice, you must choose a different approach for each, as explained in the following sections.</p>
	<h1 id="id_Toc534713651" class="block_24">Creating a simple data-driven CRUD microservice</h1>
	<p class="block_14">This section outlines how to create a simple microservice that performs create, read, update, and delete (CRUD) operations on a data source.</p>
	<h2 id="id_Toc534713652" class="block_18">Designing a simple CRUD microservice</h2>
	<p class="block_14">From a design point of view, this type of containerized microservice is very simple. Perhaps the problem to solve is simple, or perhaps the implementation is only a proof of concept.</p>
	<p class="block_14"><img src="images/image-57.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image4.png" class="calibre82"/></p>
	<p class="block_23"><span class="text_6">Figure 6-4</span><i class="calibre8">. Internal design for simple CRUD microservices</i></p>
	<p class="block_14">An example of this kind of simple data-drive service is the catalog microservice from the eShopOnContainers sample application. This type of service implements all its functionality in a single ASP.NET Core Web API project that includes classes for its data model, its business logic, and its data access code. It also stores its related data in a database running in SQL Server (as another container for dev/test purposes), but could also be any regular SQL Server host, as shown in Figure 6-5.</p>
	<p class="block_14"><img src="images/image-58.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image5.png" class="calibre83"/></p>
	<p class="block_23"><span class="text_6">Figure 6-5</span><i class="calibre8">. Simple data-driven/CRUD microservice design</i></p>
	<p class="block_17"><span class="text_5">When you are developing this kind of service, you only need </span><a href="https://docs.microsoft.com/aspnet/core/" class="text_4">ASP.NET Core</a><span class="text_5"> and a data-access API or ORM like </span><a href="https://docs.microsoft.com/ef/core/index" class="text_4">Entity Framework Core</a><span class="text_5">. You could also generate </span><a href="https://swagger.io/" class="text_4">Swagger</a><span class="text_5"> metadata automatically through </span><a href="https://github.com/domaindrivendev/Swashbuckle.AspNetCore" class="text_4">Swashbuckle</a><span class="text_5"> to provide a description of what your service offers, as explained in the next section.</span></p>
	<p class="block_14">Note that running a database server like SQL Server within a Docker container is great for development environments, because you can have all your dependencies up and running without needing to provision a database in the cloud or on-premises. This is very convenient when running integration tests. However, for production environments, running a database server in a container is not recommended, because you usually do not get high availability with that approach. For a production environment in Azure, it is recommended that you use Azure SQL DB or any other database technology that can provide high availability and high scalability. For example, for a NoSQL approach, you might choose CosmosDB.</p>
	<p class="block_14">Finally, by editing the Dockerfile and docker-compose.yml metadata files, you can configure how the image of this container will be created—what base image it will use, plus design settings such as internal and external names and TCP ports.</p>
	<h2 id="id_Toc534713653" class="block_18">Implementing a simple CRUD microservice with ASP.NET Core</h2>
	<p class="block_14">To implement a simple CRUD microservice using .NET Core and Visual Studio, you start by creating a simple ASP.NET Core Web API project (running on .NET Core so it can run on a Linux Docker host), as shown in Figure 6-6.</p>
	<p class="block_14"><img src="images/image-59.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image6.png" class="calibre84"/></p>
	<p class="block_23"><span class="text_6">Figure 6-6</span><i class="calibre8">. Creating an ASP.NET Core Web API project in Visual Studio</i></p>
	<p class="block_14">After creating the project, you can implement your MVC controllers as you would in any other Web API project, using the Entity Framework API or other API. In a new Web API project, you can see that the only dependency you have in that microservice is on ASP.NET Core itself. Internally, within the <i class="calibre15">Microsoft.AspNetCore.All</i> dependency, it is referencing Entity Framework and many other .NET Core Nuget packages, as shown in Figure 6-7.</p>
	<p class="block_14"><img src="images/image-60.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image8.png" class="calibre85"/></p>
	<p class="block_23"><span class="text_6">Figure 6-7</span><i class="calibre8">. Dependencies in a simple CRUD Web API microservice</i></p>
	<h3 id="id_Toc534713654" class="block_19">Implementing CRUD Web API services with Entity Framework Core</h3>
	<p class="block_14">Entity Framework (EF) Core is a lightweight, extensible, and cross-platform version of the popular Entity Framework data access technology. EF Core is an object-relational mapper (ORM) that enables .NET developers to work with a database using .NET objects.</p>
	<p class="block_14">The catalog microservice uses EF and the SQL Server provider because its database is running in a container with the SQL Server for Linux Docker image. However, the database could be deployed into any SQL Server, such as Windows on-premises or Azure SQL DB. The only thing you would need to change is the connection string in the ASP.NET Web API microservice.</p>
	<h4 id="id_thedatamodel" class="block_33">The data model</h4>
	<p class="block_17"><span class="text_5"><img src="images/Screen_Shot_2019-01-15_at_6.21.40_PM.png" alt="Image" class="calibre86"/>With EF Core, data access is performed by using a model. A model is made up of (domain model) entity classes and a derived context (DbContext) that represents a session with the database, allowing you to query and save data. You can generate a model from an existing database, manually code a model to match your database, or use EF migrations to create a database from your model, using the code-first approach (that makes it easy to evolve the database as your model changes over time). For the catalog microservice we are using the last approach. You can see an example of the CatalogItem entity class in the following code example, which is a simple Plain Old CLR Object (</span><a href="https://en.wikipedia.org/wiki/Plain_Old_CLR_Object" class="text_4">POCO</a><span class="text_5">) entity class.</span></p>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.22.17_PM.png" alt="Image" class="calibre87"/>You also need a DbContext that represents a session with the database. For the catalog microservice, the CatalogContext class derives from the DbContext base class, as shown in the following example:</p>
	<p class="block_27"><span class="text_8">You can have additional </span><span class="text_9">DbContext</span><span class="text_8"> implementations. For example, in the sample Catalog.API microservice, there’s a second </span><span class="text_9">DbContext</span><span class="text_8"> named </span><span class="text_9">CatalogContextSeed</span><span class="text_8"> where it automatically populates the sample data the first time it tries to access the database. This method is useful for demo data and for automated testing scenarios, as well.</span></p>
	<p class="block_15"><span class="text_12">Within the </span><span class="text_13">DbContext</span><span class="text_12">, you use the </span><span class="text_13">OnModelCreating</span><span class="text_12"> method to customize object/database entity mappings and other </span><a href="https://blogs.msdn.microsoft.com/dotnet/2016/09/29/implementing-seeding-custom-conventions-and-interceptors-in-ef-core-1-0/" class="text_14">EF extensibility points</a><span class="text_12">.</span></p>
	<h5 id="id_queryingdatafromwebapicontrollers" class="block_35">Querying data from Web API controllers</h5>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.22.45_PM.png" alt="Image" class="calibre88"/>Instances of your entity classes are typically retrieved from the database using Language Integrated Query (LINQ), as shown in the following example:</p>
	<h5 id="id_savingdata" class="block_35">Saving data</h5>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.22.54_PM.png" alt="Image" class="calibre89"/>Data is created, deleted, and modified in the database using instances of your entity classes. You could add code like the following hard-coded example (mock data, in this case) to your Web API controllers.</p>
	<h5 id="id_dependencyinjectioninaspnetcoreand" class="block_35">Dependency Injection in ASP.NET Core and Web API controllers</h5>
	<p class="block_14">In ASP.NET Core you can use Dependency Injection (DI) out of the box. You do not need to set up a third-party Inversion of Control (IoC) container, although you can plug your preferred IoC container into the ASP.NET Core infrastructure if you want. In this case, it means that you can directly inject the required EF DBContext or additional repositories through the controller constructor.</p>
	<p class="block_27"><span class="text_8">In the example above of the </span><span class="text_9">CatalogController</span><span class="text_8"> class, we are injecting an object of </span><span class="text_9">CatalogContext</span><span class="text_8"> type plus other objects through the </span><span class="text_9">CatalogController()</span><span class="text_8"> constructor.</span></p>
	<p class="block_27"><span class="text_8"><img src="images/Screen_Shot_2019-01-15_at_6.23.06_PM.png" alt="Image" class="calibre90"/>An important configuration to set up in the Web API project is the DbContext class registration into the service’s IoC container. You typically do so in the </span><span class="text_9">Startup</span><span class="text_8"> class by calling the </span><span class="text_9">services.AddDbContext()</span><span class="text_8"> method inside the </span><span class="text_9">ConfigureServices()</span><span class="text_8"> method, as shown in the following example:</span></p>
	<h3 id="id_Toc534713655" class="block_19">Additional resources</h3>
	<ul class="list_">
	<li class="block_20"><span class="text_2">Querying Data</span><span class="text_"><br class="calibre6"/></span><a href="https://docs.microsoft.com/ef/core/querying/index" class="text_1">https://docs.microsoft.com/ef/core/querying/index</a></li>
	<li class="block_20"><span class="text_2">Saving Data</span><span class="text_"><br class="calibre6"/></span><a href="https://docs.microsoft.com/ef/core/saving/index" class="text_1">https://docs.microsoft.com/ef/core/saving/index</a></li>
</ul>
	<h2 id="id_Toc534713656" class="block_18">The DB connection string and environment variables used by Docker containers</h2>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.24.18_PM.png" alt="Image" class="calibre91"/>You can use the ASP.NET Core settings and add a ConnectionString property to your settings.json file as shown in the following example:</p>
	<p class="block_14">The settings.json file can have default values for the ConnectionString property or for any other property. However, those properties will be overridden by the values of environment variables that you specify in the docker-compose.override.yml file, when using Docker.</p>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.24.25_PM.png" alt="Image" class="calibre92"/>From your docker-compose.yml or docker-compose.override.yml files, you can initialize those environment variables so that Docker will set them up as OS environment variables for you, as shown in the following docker-compose.override.yml file (the connection string and other lines wrap in this example, but it would not wrap in your own file).</p>
	<p class="block_14">The docker-compose.yml files at the solution level are not only more flexible than configuration files at the project or microservice level, but also more secure if you override the environment variables declared at the docker-compose files with values set from your deployment tools, like from Azure DevOps Services Docker deployment tasks.</p>
	<p class="block_27"><span class="text_8">Finally, you can get that value from your code by using </span><span class="text_9">Configuration[“ConnectionString”]</span><span class="text_8">, as shown in the ConfigureServices method in an earlier code example.</span></p>
	<p class="block_17"><span class="text_5">However, for production environments, you might want to explore additional ways on how to store secrets like the connection strings. An excellent way to manage application secrets is using </span><a href="https://azure.microsoft.com/services/key-vault/" class="text_4">Azure Key Vault</a><span class="text_5">.</span></p>
	<p class="block_17"><span class="text_5">Azure Key Vault helps to store and safeguard cryptographic keys and secrets used by your cloud applications and services. A secret is anything you want to keep strict control of, like API keys, connection strings, passwords, etc. and strict control includes usage logging, setting expiration, managing access, </span><a href="https://docs.microsoft.com/azure/key-vault/about-keys-secrets-and-certificates" class="text_4">among others</a><span class="text_5">.</span></p>
	<p class="block_14">Azure Key Vault allows a very detailed control level of the application secrets usage without the need to let anyone know them. The secrets can even be rotated for enhanced security without disrupting development or operations.</p>
	<p class="block_14">Applications have to be registered in the organization’s Active Directory, so they can use the Key Vault.</p>
	<p class="block_17"><span class="text_5">You can check the </span><a href="https://docs.microsoft.com/azure/key-vault/key-vault-whatis" class="text_4">Key Vault Concepts documentation</a><span class="text_5"> for more details.</span></p>
	<h3 id="id_Toc534713657" class="block_19">Implementing versioning in ASP.NET Web APIs</h3>
	<p class="block_14">As business requirements change, new collections of resources may be added, the relationships between resources might change, and the structure of the data in resources might be amended. Updating a Web API to handle new requirements is a relatively straightforward process, but you must consider the effects that such changes will have on client applications consuming the Web API. Although the developer designing and implementing a Web API has full control over that API, the developer does not have the same degree of control over client applications that might be built by third party organizations operating remotely.</p>
	<p class="block_14">Versioning enables a Web API to indicate the features and resources that it exposes. A client application can then submit requests to a specific version of a feature or resource. There are several approaches to implement versioning:</p>
	<ul class="list_">
	<li class="block_25">URI versioning</li>
	<li class="block_25">Query string versioning</li>
	<li class="block_25">Header versioning</li>
</ul>
	<p class="block_14">Query string and URI versioning are the simplest to implement. Header versioning is a good approach. However, header versioning not as explicit and straightforward as URI versioning. Because URL versioning is the simplest and most explicit, the eShopOnContainers sample application uses URI versioning.</p>
	<p class="block_14">With URI versioning, as in the eShopOnContainers sample application, each time you modify the Web API or change the schema of resources, you add a version number to the URI for each resource. Existing URIs should continue to operate as before, returning resources that conform to the schema that matches the requested version.</p>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.24.34_PM.png" alt="Image" class="calibre93"/>As shown in the following code example, the version can be set by using the Route attribute in the Web API controller, which makes the version explicit in the URI (v1 in this case).</p>
	<p class="block_17"><span class="text_5">This versioning mechanism is simple and depends on the server routing the request to the appropriate endpoint. However, for a more sophisticated versioning and the best method when using REST, you should use hypermedia and implement </span><a href="https://docs.microsoft.com/azure/architecture/best-practices/api-design" class="text_4">HATEOAS (Hypertext as the Engine of Application State)</a><span class="text_5">.</span></p>
	<h3 id="id_Toc534713658" class="block_19">Additional resources</h3>
	<ul class="list_">
	<li class="block_20"><span class="text_2">Scott Hanselman. ASP.NET Core RESTful Web API versioning made easy</span><span class="text_"><br class="calibre6"/></span><a href="https://www.hanselman.com/blog/ASPNETCoreRESTfulWebAPIVersioningMadeEasy.aspx" class="text_1">https://www.hanselman.com/blog/ASPNETCoreRESTfulWebAPIVersioningMadeEasy.aspx</a></li>
	<li class="block_20"><span class="text_2">Versioning a RESTful web API</span><span class="text_"><br class="calibre6"/></span><a href="https://docs.microsoft.com/azure/architecture/best-practices/api-design" class="text_1">https://docs.microsoft.com/azure/architecture/best-practices/api-design#versioning-a-restful-web-api</a></li>
	<li class="block_20"><span class="text_2">Roy Fielding. Versioning, Hypermedia, and REST</span><span class="text_"><br class="calibre6"/></span><a href="https://www.infoq.com/articles/roy-fielding-on-versioning" class="text_1">https://www.infoq.com/articles/roy-fielding-on-versioning</a></li>
</ul>
	<h2 id="id_Toc534713659" class="block_18">Generating Swagger description metadata from your ASP.NET Core Web API</h2>
	<p class="block_17"><a href="https://swagger.io/" class="text_4">Swagger</a><span class="text_5"> is a commonly used open source framework backed by a large ecosystem of tools that helps you design, build, document, and consume your RESTful APIs. It is becoming the standard for the APIs description metadata domain. You should include Swagger description metadata with any kind of microservice, either data-driven microservices or more advanced domain-driven microservices (as explained in following section).</span></p>
	<p class="block_14">The heart of Swagger is the Swagger specification, which is API description metadata in a JSON or YAML file. The specification creates the RESTful contract for your API, detailing all its resources and operations in both a human- and machine-readable format for easy development, discovery, and integration.</p>
	<p class="block_14">The specification is the basis of the OpenAPI Specification (OAS) and is developed in an open, transparent, and collaborative community to standardize the way RESTful interfaces are defined.</p>
	<p class="block_17"><span class="text_5">The specification defines the structure for how a service can be discovered and how its capabilities understood. For more information, including a web editor and examples of Swagger specifications from companies like Spotify, Uber, Slack, and Microsoft, see the Swagger site (</span><a href="https://swagger.io" class="text_4">https://swagger.io</a><span class="text_5">).</span></p>
	<h3 id="id_Toc534713660" class="block_19">Why use Swagger?</h3>
	<p class="block_14">The main reasons to generate Swagger metadata for your APIs are the following.</p>
	<p class="block_17"><span class="text_7">Ability for other products to automatically consume and integrate your APIs</span><span class="text_5">. Dozens of products and </span><a href="https://swagger.io/commercial-tools/" class="text_4">commercial tools</a><span class="text_5"> and many </span><a href="https://swagger.io/open-source-integrations/" class="text_4">libraries and frameworks</a><span class="text_5"> support Swagger. Microsoft has high-level products and tools that can automatically consume Swagger-based APIs, such as the following:</span></p>
	<ul class="list_">
	<li class="block_20"><a href="https://github.com/Azure/AutoRest" class="text_1">AutoRest</a><span class="text_">. You can automatically generate .NET client classes for calling Swagger. This tool can be used from the CLI and it also integrates with Visual Studio for easy use through the GUI.</span></li>
	<li class="block_20"><a href="https://flow.microsoft.com/en-us/" class="text_1">Microsoft Flow</a><span class="text_">. You can automatically </span><a href="https://flow.microsoft.com/en-us/blog/integrating-custom-api/" class="text_1">use and integrate your API</a><span class="text_"> into a high-level Microsoft Flow workflow, with no programming skills required.</span></li>
	<li class="block_20"><a href="https://powerapps.microsoft.com/" class="text_1">Microsoft PowerApps</a><span class="text_">. You can automatically consume your API from </span><a href="https://powerapps.microsoft.com/blog/register-and-use-custom-apis-in-powerapps/" class="text_1">PowerApps mobile apps</a><span class="text_"> built with </span><a href="https://powerapps.microsoft.com/build-powerapps/" class="text_1">PowerApps Studio</a><span class="text_">, with no programming skills required.</span></li>
	<li class="block_20"><a href="https://docs.microsoft.com/azure/app-service-logic/app-service-logic-what-are-logic-apps" class="text_1">Azure App Service Logic Apps</a><span class="text_">. You can automatically </span><a href="https://docs.microsoft.com/azure/app-service-logic/app-service-logic-custom-hosted-api" class="text_1">use and integrate your API into an Azure App Service Logic App</a><span class="text_">, with no programming skills required.</span></li>
</ul>
	<p class="block_14"><b class="calibre5">Ability to automatically generate API documentation</b>. When you create large-scale RESTful APIs, such as complex microservice-based applications, you need to handle many endpoints with different data models used in the request and response payloads. Having proper documentation and having a solid API explorer, as you get with Swagger, is key for the success of your API and adoption by developers.</p>
	<p class="block_14">Swagger’s metadata is what Microsoft Flow, PowerApps, and Azure Logic Apps use to understand how to use APIs and connect to them.</p>
	<p class="block_14">There are several options to automate Swagger metadata generation for ASP.NET Core REST API applications, in the form of functional API help pages, based on <span class="text_18">swagger-ui</span>.</p>
	<p class="block_17"><span class="text_5">Probably the best know is </span><a href="https://github.com/domaindrivendev/Swashbuckle.AspNetCore" class="text_4">Swashbuckle</a><span class="text_5"> which is currently used in </span><a href="https://github.com/dotnet-architecture/eShopOnContainers" class="text_4">eShopOnCntainers</a><span class="text_5"> and we’ll cover in some detail in this guide but there’s also the option to use </span><a href="https://github.com/RSuter/NSwag" class="text_4">NSwag</a><span class="text_5">, that can generate Typescript and C# API clients, as well as C# controllers, from a Swagger or OpenAPI specification and even by scanning the .dll that contains the controllers, using </span><a href="https://github.com/RSuter/NSwag/wiki/NSwagStudio" class="text_4">NSwagStudio</a><span class="text_5">.</span></p>
	<h3 id="id_Toc534713661" class="block_19">How to automate API Swagger metadata generation with the Swashbuckle NuGet package</h3>
	<p class="block_17"><span class="text_5">Generating Swagger metadata manually (in a JSON or YAML file) can be tedious work. However, you can automate API discovery of ASP.NET Web API services by using the </span><a href="https://aka.ms/swashbuckledotnetcore" class="text_4">Swashbuckle NuGet package</a><span class="text_5"> to dynamically generate Swagger API metadata.</span></p>
	<p class="block_14">Swashbuckle automatically generates Swagger metadata for your ASP.NET Web API projects. It supports ASP.NET Core Web API projects and the traditional ASP.NET Web API and any other flavor, such as Azure API App, Azure Mobile App, Azure Service Fabric microservices based on ASP.NET. It also supports plain Web API deployed on containers, as in for the reference application.</p>
	<p class="block_17"><span class="text_5">Swashbuckle combines API Explorer and Swagger or </span><a href="https://github.com/swagger-api/swagger-ui" class="text_4">swagger-ui</a><span class="text_5"> to provide a rich discovery and documentation experience for your API consumers. In addition to its Swagger metadata generator engine, Swashbuckle also contains an embedded version of swagger-ui, which it will automatically serve up once Swashbuckle is installed.</span></p>
	<p class="block_14">This means you can complement your API with a nice discovery UI to help developers to use your API. It requires a very small amount of code and maintenance because it is automatically generated, allowing you to focus on building your API. The result for the API Explorer looks like Figure 6-8.</p>
	<p class="block_14"><img src="images/image-61.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image9.png" class="calibre94"/></p>
	<p class="block_23"><span class="text_6">Figure 6-8</span><i class="calibre8">. Swashbuckle API Explorer based on Swagger metadata—eShopOnContainers catalog microservice</i></p>
	<p class="block_17"><span class="text_5">The API explorer is not the most important thing here. Once you have a Web API that can describe itself in Swagger metadata, your API can be used seamlessly from Swagger-based tools, including client proxy-class code generators that can target many platforms. For example, as mentioned, </span><a href="https://github.com/Azure/AutoRest" class="text_4">AutoRest</a><span class="text_5"> automatically generates .NET client classes. But additional tools like </span><a href="https://github.com/swagger-api/swagger-codegen" class="text_4">swagger-codegen</a><span class="text_5"> are also available, which allow code generation of API client libraries, server stubs, and documentation automatically.</span></p>
	<p class="block_17"><span class="text_5">Currently, Swashbuckle consists of five internal NuGet packages under the high-level meta- package </span><a href="https://www.nuget.org/packages/Swashbuckle.AspNetCore" class="text_4">Swashbuckle.AspNetCore</a><span class="text_5"> for ASP.NET Core applications.</span></p>
	<p class="block_14">After you have installed these NuGet packages in your Web API project, you need to configure Swagger in the Startup class, as in the following code (simplified):</p>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.25.43_PM.png" alt="Image" class="calibre95"/><img src="images/Screen_Shot_2019-01-15_at_6.25.36_PM.png" alt="Image" class="calibre96"/>Once this is done, you can start your application and browse the following Swagger JSON and UI endpoints using URLs like these:</p>
	<p class="block_14">You previously saw the generated UI created by Swashbuckle for a URL like http://&lt;your-root-url&gt;/swagger. In Figure 6-9 you can also see how you can test any API method.</p>
	<p class="block_14"><img src="images/image-62.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image10.png" class="calibre97"/></p>
	<p class="block_23"><span class="text_6">Figure 6-9</span><i class="calibre8">. Swashbuckle UI testing the Catalog/Items API method</i></p>
	<p class="block_17"><span class="text_5">Figure 6-10 shows the Swagger JSON metadata generated from the eShopOnContainers microservice (which is what the tools use underneath) when you request &lt;your-root-url&gt;/swagger/v1/swagger.json using </span><a href="https://www.getpostman.com/" class="text_4">Postman</a><span class="text_5">.</span></p>
	<p class="block_14"><img src="images/image-63.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image11.png" class="calibre98"/></p>
	<p class="block_23"><span class="text_6">Figure 6-10</span><i class="calibre8">. Swagger JSON metadata</i></p>
	<p class="block_14">It is that simple. And because it is automatically generated, the Swagger metadata will grow when you add more functionality to your API.</p>
	<h3 id="id_Toc534713662" class="block_19">Additional resources</h3>
	<ul class="list_">
	<li class="block_20"><span class="text_2">ASP.NET Web API Help Pages using Swagger</span><span class="text_"><br class="calibre6"/></span><a href="https://docs.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swagger" class="text_1">https://docs.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swagger</a></li>
	<li class="block_20"><span class="text_2">Get started with Swashbuckle and ASP.NET Core</span><span class="text_"><br class="calibre6"/></span><a href="https://docs.microsoft.com/aspnet/core/tutorials/getting-started-with-swashbuckle?tabs=visual-studio" class="text_1">https://docs.microsoft.com/aspnet/core/tutorials/getting-started-with-swashbuckle?tabs=visual-studio</a></li>
	<li class="block_20"><span class="text_2">Get started with NSwag and ASP.NET Core</span><span class="text_"><br class="calibre6"/></span><a href="https://docs.microsoft.com/aspnet/core/tutorials/getting-started-with-nswag?tabs=visual-studio" class="text_1">https://docs.microsoft.com/aspnet/core/tutorials/getting-started-with-nswag?tabs=visual-studio</a></li>
</ul>
	<h1 id="id_Toc534713663" class="block_24">Defining your multi-container application with docker-compose.yml</h1>
	<p class="block_17"><span class="text_5">In this guide, the </span><a href="https://docs.docker.com/compose/compose-file/" class="text_4">docker-compose.yml</a><span class="text_5"> file was introduced in the section </span><a class="text_4">Step 4. Define your services in docker-compose.yml when building a multi-container Docker application</a><span class="text_5">. However, there are additional ways to use the docker-compose files that are worth exploring in further detail.</span></p>
	<p class="block_14">For example, you can explicitly describe how you want to deploy your multi-container application in the docker-compose.yml file. Optionally, you can also describe how you are going to build your custom Docker images. (Custom Docker images can also be built with the Docker CLI.)</p>
	<p class="block_15"><span class="text_12">Basically, you define each of the containers you want to deploy plus certain characteristics for each container deployment. Once you have a multi-container deployment description file, you can deploy the whole solution in a single action orchestrated by the </span><a href="https://docs.docker.com/compose/overview/" class="text_14">docker-compose up</a><span class="text_12"> CLI command, or you can deploy it transparently from Visual Studio. Otherwise, you would need to use the Docker CLI to deploy container-by-container in multiple steps by using the </span><span class="text_13">docker run</span><span class="text_12"> command from the command line. Therefore, each service defined in docker-compose.yml must specify exactly one image or build. Other keys are optional, and are analogous to their </span><span class="text_13">docker run</span><span class="text_12"> command-line counterparts.</span></p>
	<p class="block_14">The following YAML code is the definition of a possible global but single docker-compose.yml file for the eShopOnContainers sample. This is not the actual docker-compose file from eShopOnContainers. Instead, it is a simplified and consolidated version in a single file, which is not the best way to work with docker-compose files, as will be explained later.</p>
	<p class="block_15"> </p>
	<p class="block_15"> </p>
	<p class="block_15"> </p>
	<p class="block_15"> </p>
	<p class="block_15"> </p>
	<p class="block_15"> </p>
	<p class="block_27"><span class="text_8"><img src="images/Screen_Shot_2019-01-15_at_6.29.43_PM.png" alt="Image" class="calibre99"/>The root key in this file is services. Under that key you define the services you want to deploy and run when you execute the </span><span class="text_9">docker-compose up</span><span class="text_8"> command or when you deploy from Visual Studio by using this docker-compose.yml file. In this case, the docker-compose.yml file has multiple services defined, as <img src="images/Screen_Shot_2019-01-15_at_6.30.31_PM.png" alt="Image" class="calibre100"/>described in the following table.</span></p>
	<h3 id="id_Toc534713664" class="block_19">A simple Web Service API container</h3>
	<p class="block_14"><img src="images/Screen_Shot_2019-01-15_at_6.30.49_PM.png" alt="Image" class="calibre101"/>Focusing on a single container, the catalog.api container-microservice has a straightforward definition:</p>
	<p class="block_14">This containerized service has the following basic configuration:</p>
	<ul class="list_">
	<li class="block_25">It is based on the custom eshop/catalog.api image. For simplicity’s sake, there is no build: key setting in the file. This means that the image must have been previously built (with docker build) or have been downloaded (with the docker pull command) from any Docker registry.</li>
	<li class="block_25">It defines an environment variable named ConnectionString with the connection string to be used by Entity Framework to access the SQL Server instance that contains the catalog data model. In this case, the same SQL Server container is holding multiple databases. Therefore, you need less memory in your development machine for Docker. However, you could also deploy one SQL Server container for each microservice database.</li>
	<li class="block_25">The SQL Server name is sql.data, which is the same name used for the container that is running the SQL Server instance for Linux. This is convenient; being able to use this name resolution (internal to the Docker host) will resolve the network address so you don’t need to know the internal IP for the containers you are accessing from other containers.</li>
</ul>
	<p class="block_14">Because the connection string is defined by an environment variable, you could set that variable through a different mechanism and at a different time. For example, you could set a different connection string when deploying to production in the final hosts, or by doing it from your CI/CD pipelines in Azure DevOps Services or your preferred DevOps system.</p>
	<ul class="list_">
	<li class="block_25">It exposes port 80 for internal access to the catalog.api service within the Docker host. The host is currently a Linux VM because it is based on a Docker image for Linux, but you could configure the container to run on a Windows image instead.</li>
	<li class="block_25">It forwards the exposed port 80 on the container to port 5101 on the Docker host machine (the Linux VM).</li>
	<li class="block_25">It links the web service to the sql.data service (the SQL Server instance for Linux database running in a container). When you specify this dependency, the catalog.api container will not start until the sql.data container has already started; this is important because catalog.api needs to have the SQL Server database up and running first. However, this kind of container dependency is not enough in many cases, because Docker checks only at the container level. Sometimes the service (in this case SQL Server) might still not be ready, so it is advisable to implement retry logic with exponential backoff in your client microservices. That way, if a dependency container is not ready for a short time, the application will still be resilient.</li>
	<li class="block_25">It is configured to allow access to external servers: the extra_hosts setting allows you to access external servers or machines outside of the Docker host (that is, outside the default Linux VM which is a development Docker host), such as a local SQL Server instance on your development PC.</li>
</ul>
	<p class="block_14">There are also other, more advanced docker-compose.yml settings that we will discuss in the following sections.</p>
	<h3 id="id_Toc534713665" class="block_19">Using docker-compose files to target multiple environments</h3>
	<p class="block_14">The docker-compose.yml files are definition files and can be used by multiple infrastructures that understand that format. The most straightforward tool is the docker-compose command.</p>
	<p class="block_14">Therefore, by using the docker-compose command you can target the following main scenarios.</p>
	<h4 id="id_developmentenvironments" class="block_33">Development environments</h4>
	<p class="block_14">When you develop applications, it is important to be able to run an application in an isolated development environment. You can use the docker-compose CLI command to create that environment or use Visual Studio which uses docker-compose under the covers.</p>
	<p class="block_14">The docker-compose.yml file allows you to configure and document all your application’s service dependencies (other services, cache, databases, queues, etc.). Using the docker-compose CLI command, you can create and start one or more containers for each dependency with a single command (docker-compose up).</p>
	<p class="block_14">The docker-compose.yml files are configuration files interpreted by Docker engine but also serve as convenient documentation files about the composition of your multi-container application.</p>
	<h4 id="id_testingenvironments" class="block_33">Testing environments</h4>
	<p class="block_14">An important part of any continuous deployment (CD) or continuous integration (CI) process are the unit tests and integration tests. These automated tests require an isolated environment so they are not impacted by the users or any other change in the application’s data.</p>
	<p class="block_14"><img src="images/image101.tiff" alt="Image" class="calibre102"/>With Docker Compose you can create and destroy that isolated environment very easily in a few commands from your command prompt or scripts, like the following commands:</p>
	<h4 id="id_productiondeployments" class="block_33">Production deployments</h4>
	<p class="block_17"><span class="text_5">You can also use Compose to deploy to a remote Docker Engine. A typical case is to deploy to a single Docker host instance (like a production VM or server provisioned with </span><a href="https://docs.docker.com/machine/overview/" class="text_4">Docker Machine</a><span class="text_5">).</span></p>
	<p class="block_14">If you are using any other orchestrator (Azure Service Fabric, Kubernetes, etc.), you might need to add setup and metadata configuration settings like those in docker-compose.yml, but in the format required by the other orchestrator. </p>
	<p class="block_14">In any case, docker-compose is a convenient tool and metadata format for development, testing and production workflows, although the production workflow might vary on the orchestrator you are using.</p>
	<h3 id="id_Toc534713666" class="block_19">Using multiple docker-compose files to handle several environments</h3>
	<p class="block_14">When targeting different environments, you should use multiple compose files. This lets you create multiple configuration variants depending on the environment.</p>
	<h4 id="id_overridingthebasedockercomposefile" class="block_33">Overriding the base docker-compose file</h4>
	<p class="block_14">You could use a single docker-compose.yml file as in the simplified examples shown in previous sections. However, that is not recommended for most applications.</p>
	<p class="block_14">By default, Compose reads two files, a docker-compose.yml and an optional docker-compose.override.yml file. As shown in Figure 6-11, when you are using Visual Studio and enabling Docker support, Visual Studio also creates an additional docker-compose.vs.debug.g.yml file for debugging the application, you can take a look at this file in folder obj\Docker\ in the main solution folder.</p>
	<p class="block_14"><img src="images/image-64.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image12.png" class="calibre103"/></p>
	<p class="block_23"><span class="text_6">Figure 6-11</span><i class="calibre8">. docker-compose files in Visual Studio 2017</i></p>
	<p class="block_14">You can edit the docker-compose files with any editor, like Visual Studio Code or Sublime, and run the application with the docker-compose up command.</p>
	<p class="block_14">By convention, the docker-compose.yml file contains your base configuration and other static settings. That means that the service configuration should not change depending on the deployment environment you are targeting.</p>
	<p class="block_14">The docker-compose.override.yml file, as its name suggests, contains configuration settings that override the base configuration, such as configuration that depends on the deployment environment. You can have multiple override files with different names also. The override files usually contain additional information needed by the application but specific to an environment or to a deployment.</p>
	<h4 id="id_targetingmultipleenvironments" class="block_33">Targeting multiple environments</h4>
	<p class="block_14">A typical use case is when you define multiple compose files so you can target multiple environments, like production, staging, CI, or development. To support these differences, you can split your Compose configuration into multiple files, as shown in Figure 6-12.</p>
	<p class="block_14"><img src="images/image-65.png" alt="C:\Users\Miguel\source\repos\dotnet\docs\docs\standard\microservices-architecture\multi-container-microservice-net-applications\media\image13.png" class="calibre104"/></p>
	<p class="block_23"><span class="text_6">Figure 6-12</span><i class="calibre8">. Multiple docker-compose files overriding values in the base docker-compose.yml file</i></p>
	<p class="block_14">You start with the base docker-compose.yml file. This base file has to contain the base or static configuration settings that do not change depending on the environment. For example, the eShopOnContainers has the following docker-compose.yml file (simplified with less services) as the base file.</p>
	<p class="block_4" id="calibre_pb_8"> </p>
</body></html>
